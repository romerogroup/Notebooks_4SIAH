{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning for Scientists in a hurry](./fig/Title.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chapter_number = 1\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Road to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is one of the techniques in Machine Learning with most success in a variety of problems. From Classification to Regression. Its ability to account for complexity is remarkable.\n",
    "\n",
    "Deep Learning is one of the most active areas in Machine Learning today. However, Neural networks have been around for decades, so the question is why now?\n",
    "As we will learn in this workshop the hype created around Deep Learning in the last decade is due to the convergence of a set of elements that propelled Deep Learning from an obscure topic in Machine Learning to the driving force of many of our activities in daily life. We can summarize those elements as:\n",
    "\n",
    "   * The availability of large datasets, thanks in part to the internet and big tech companies.\n",
    "   \n",
    "   * Powerful accelerators such as GPUs that are capable of processing data for some tasks (Linear Algebra) at scales outperform CPUs by at least one order of magnitude.\n",
    "\n",
    "\n",
    "Every time you are using your phone or smart TV, computer, neural networks are working for you. They have become almost both ubiquitous and invisible as electricity became from the beginning of the 20th century. In a way, Machine Learning and Deep Learning are the fuel of some sort of new industrial revolution or at least with the potential to become something like it. \n",
    "\n",
    "## Deep Learning, Machine Learning, and Artificial Intelligence\n",
    "\n",
    "Deep Learning is part of the area of Machine Learning, a set of techniques to produce models that use examples to build the model, rather than using a hardcoded algorithm. \n",
    "Machine Learning itself is just an area of Artificial Intelligence, an area of computer science dedicated to studying how computers can perform tasks usually considered as intellectual.\n",
    "\n",
    "<div>\n",
    "<img src=\"./fig/AI-ML-DL.pdf\" width=\"400\" >\n",
    "</div>\n",
    "\n",
    "### Learning from data, a scientific perspective\n",
    "\n",
    "The idea of learning from data is not foreign to scientist.\n",
    "The objective of science is to create models that explain phenomena and provides predictions that can be confirmed or rejected by experiments or observations.\n",
    "\n",
    "Scientists create models that not only give us insight about nature but also equations that allow us to make predictions.  In most cases, clean equations are simply not possible and we have to use numerical approximations but we try to keep the understanding. Machine Learning is used in cases where mathematical models are known, numerical approximations are not feasible, and we\n",
    "We are satisfied with the answers even if we lost the ability to understand why the parameters of Machine Learning models work the way they do.\n",
    "\n",
    "In summary, we need 3 conditions for using Machine Learning on a problem:\n",
    "\n",
    " * Good **data**\n",
    " * The existence of **patterns** on the data\n",
    " * The lack of a good mathematical model to express the **patterns** present on the **data**\n",
    "\n",
    "### Kepler, an example of learning from data\n",
    "\n",
    "<div>\n",
    "\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/Stamps_of_Germany_(DDR)_1971,_MiNr_1649.jpg\" width=\"400\" ></th>\n",
    "    <th><img src=\"./fig/keplertableslrg.jpg\" width=\"400\" ></th>\n",
    "  </tr>\n",
    "  </table>\n",
    "  \n",
    "  <table>\n",
    "  <tr>\n",
    "  <th><img src=\"./fig/Kepler-1627-60-61.jpg\" width=\"800\" >\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <th><img src=\"./fig/Rudolphine_tables.jpg\" width=\"800\" >\n",
    "  </tr>\n",
    "  </table>\n",
    "\n",
    "</div>\n",
    "    \n",
    "## The two shores of Deep Learning\n",
    "\n",
    "There are two ways to approach Deep Learning. The biological side and the mathematical side.\n",
    "\n",
    "An Artificial Neural Network (ANN) is a computational model that is **loosely** inspired by the way biological neural networks in the animal brain process information.\n",
    "\n",
    "From the other side, they can also be considered as a generalization of the Perceptron idea. A mathematical model \n",
    "for producing functions capable of getting close to a target function via an iterative process.\n",
    "\n",
    "Both shores serve as good analogies if we are careful not to extrapolate the original ideas beyond their regime of validity. Deep Learning is not pretending to be models of the brain and the complexity of Deep Neural networks is far beyond that of what perceptrons were capable of doing. \n",
    "\n",
    "Let's explore these two approaches for a moment.\n",
    "\n",
    "\n",
    "### Biological Neural Networks\n",
    "\n",
    "From one side the idea of simulating synapsis in biological Neural Networks and using the knowledge about activation barriers and multiple connectivities as inspiration to create an Artificial Neural Network. The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10¹⁴ — 10¹⁵ synapses\n",
    "\n",
    "![Biological to Artificial Neural Networks](./fig/bioNN.png)\n",
    "\n",
    "The idea with ANN is that synaptic strengths (the weights w in our mathematical model) are learnable and control the strength of influence and its direction: excitatory (positive weight) or inhibitory (negative weight) of one neuron on another. If the final sum of the different connections is above a certain threshold, the neuron can fire, sending a spike along its axon, which is the output of the network under the provided input.\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The other origin is the idea of Perceptron in Machine Learning. \n",
    "\n",
    "Perceptron is a linear classifier (binary) and used in supervised learning. It helps to classify the given input data. As a binary classifier, it can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. \n",
    "\n",
    "<img src=\"./fig/perceptron.png\" width=\"700\" height=400>\n",
    "\n",
    "## Complexity of Neural Networks\n",
    "\n",
    "![Complexity](./fig/shallow_and_deep_NN.png)\n",
    "\n",
    "When the neural network has no hidden layer it is just a linear classifier or Perceptron. When hidden layers are added the NN can account for non-linearity, in the case of multiple hidden layers have what is called a Deep Neural Network.\n",
    "\n",
    "## In practice how complex it can be\n",
    "\n",
    "A Deep Learning model can include:\n",
    "    \n",
    " * **Input** (With many neurons)\n",
    " * **Layer 1** \n",
    " * ...\n",
    " * ...\n",
    " * **Layer N**\n",
    " * **Output layer** (With many neurons)\n",
    "    \n",
    "For example, the input can be an image with thousands of pixels and 3 colors for each pixel. Hundreds of hidden layers and the output could also be an image. That complexity is responsible for the computational cost of running such networks.\n",
    "\n",
    "##  Non-linear transformations with Deep Networks\n",
    "\n",
    "Consider this ilustrative example from this [blog](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/):\n",
    "We have two curves, as they are there is no way of creating a straight line that separates both curves.\n",
    "There is however a curve capable of separating the space in two regions where each curve lives on its region.\n",
    "Neural networks approach the problem by transforming the space in a non-linear way, allowing the two curves to be easily separable with a simple line.\n",
    "\n",
    "<div>\n",
    "    <br>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/olah_simple2_data.png\" width=\"400\" /></th>\n",
    "    <th><img src=\"./fig/olah_simple2_linear.png\" width=\"400\" /></th>\n",
    "    <th><img src=\"./fig/olah_example_network.svg\" width=\"400\" /></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/olah_simple2_0.png\" width=\"400\" /></th>\n",
    "    <th><img src=\"./fig/olah_simple2_1.png\" width=\"400\" /></th>\n",
    "  </tr>\n",
    "      </table>\n",
    "</div>\n",
    "\n",
    "## Dynamic visualization of the transformations\n",
    "     \n",
    "In its basic form, a neural network consists of the application of affine transformations (scalings, skewings and rotations, and translations) followed by pointwise application of a non-linear function:\n",
    "     \n",
    "<div>\n",
    "    <br>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/olah_1layer.gif\" width=\"400\" /></th>\n",
    "  </tr>\n",
    "  </table>\n",
    "</div>      \n",
    "\n",
    "<div>\n",
    "  <table>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/olah_spiral.1-2.2-2-2-2-2-2.gif\" width=\"400\" /></th> \n",
    "    <th><img src=\"./fig/olah_spiral.2.2-2-2-2-2-2-2.gif\" width=\"400\" /></th>\n",
    "  </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Basic Neural Network Architecture\n",
    "\n",
    "![Basic Architecture](./fig/basic_architeture.png)\n",
    "\n",
    "## Neural Network Zoo\n",
    "\n",
    "Since neural networks are one of the more active research fields in machine learning, a large number of modifications have been proposed. In the following figure, a summary of the different node structures is drawn and from that, relations and acronyms are provided such that some of the different networks are related someway. The figure below shows a summary but let me give you a quick overview of a few of them.\n",
    "\n",
    "1)  Feed forward neural networks (FF or FFNN) and perceptrons (P). They feed information from the front to the back (input and output, respectively). Neural networks are often described as having layers, where each layer consists of either input, hidden, or output cells in parallel. A layer alone never has connections and in general two adjacent layers are fully connected (every neuron from one layer to every neuron to another layer). One usually trains FFNNs through back-propagation, giving the network paired datasets of “what goes in” and “what we want to have coming out”.  Given that the network has enough hidden neurons, it can theoretically always model the relationship between the input and output. Practically their use is a lot more limited but they are popularly combined with other networks to form new networks.\n",
    "\n",
    "2) Radial basis functions. This network is simpler than the normal FFNN, as the activation function is a radial function. Each RBFN neuron stores a “prototype”, which is just one of the examples from the training set. When we want to classify a new input, each neuron computes the Euclidean distance between the input and its prototype. Roughly speaking, if the input more closely resembles the class A prototypes than the class B prototypes, it is classified as class A.\n",
    "\n",
    "3) Recurrent Neural Networks (RNN). These networks are designed to take a series of inputs with no predetermined limit on size. They are networks with loops in them, allowing information to persist.\n",
    "\n",
    "4) Long / short term memory (LSTM) networks. There is a special kind of RNN where each neuron has a memory cell and three gates: input, output, and forget. The idea of each gate is to allow or stop the flow of information through them. \n",
    "\n",
    "5) Gated recurrent units (GRU) are a slight variation on LSTMs. They have one less gate and are wired slightly differently: instead of an input, output, and forget gate, they have an update gate.\n",
    "\n",
    "6) Convolutional Neural Networks (ConvNet) is very similar to FFNN, they are made up of neurons that have learnable weights and biases. In a convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) the unit connectivity pattern is inspired by the organization of the visual cortex, Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap, over-covering the entire visual field. The unit response can be approximated mathematically by a convolution operation. They are variations of multilayer perceptrons that use minimal preprocessing. Their wide applications are in image and video recognition, recommender systems, and natural language processing. CNN's requires large data to train on.\n",
    "\n",
    "From <http://www.asimovinstitute.org/neural-network-zoo>\n",
    "    \n",
    "![Neural Network Zoo](./fig/NeuralNetworkZoo20042019.png)\n",
    "    \n",
    "    \n",
    "\n",
    "## Activation Function\n",
    "\n",
    "Each internal neuron receives input from several other neurons, computes the aggregate, and propagates the result based on the activation function. \n",
    "\n",
    "![Neural Network](./fig/nn.png)\n",
    "\n",
    "\n",
    "![Activation Function](./fig/activation_function.png)\n",
    "\n",
    "Neurons apply activation functions at these summed inputs. \n",
    "\n",
    "Activation functions are typically non-linear.\n",
    "\n",
    " * The **Sigmoid Function** produces a value between 0 and 1, so it is intuitive when a probability is desired and was almost standard for many years.\n",
    " \n",
    " * The **Rectified Linear (ReLU)** activation function is zero when the input is negative and is equal to the input when the input is positive. Rectified Linear activation functions are currently the most popular activation function as they are more efficient than the sigmoid or hyperbolic tangent.\n",
    " \n",
    "     * Sparse activation: In a randomly initialized network, only 50% of hidden units are active.\n",
    "     \n",
    "     * Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\n",
    "     \n",
    "     * Efficient computation: Only comparison, addition, and multiplication.\n",
    "     \n",
    "     * There are Leaky and Noisy variants.\n",
    "     \n",
    " * The **Soft Plus** shared some of the nice properties of ReLU and still preserves continuity on the derivative.\n",
    "\n",
    "## Inference or Foward Propagation\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th><img src=\"./fig/DL1.png\" alt=\"Input\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"./fig/DL2.png\" alt=\"Hidden\" style=\"width:400px\"></th>\n",
    "    <th><img src=\"./fig/DL3.png\" alt=\"Output\" style=\"width:400px\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Receiving Input</td>\n",
    "    <td>Computing Hidden Layer</td>\n",
    "    <td>Computing Output</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "### Receiving Input\n",
    "\n",
    " * H1 Weights = (1.0, -2.0, 2.0)\n",
    " * H2 Weights = (2.0, 1.0, -4.0)\n",
    " * H3 Weights = (1.0, -1.0, 0.0)\n",
    " * O1 Weights = (-3.0, 1.0, -3.0)\n",
    " * O2 Weights = (0.0, 1.0, 2.0)\n",
    " \n",
    "### Hidden Layer\n",
    "\n",
    " * H1 = Sigmoid(0.5 * 1.0 + 0.9 * -2.0 + -0.3 * 2.0) = Sigmoid(-1.9) = .13\n",
    " * H2 = Sigmoid(0.5 * 2.0 + 0.9 * 1.0 + -0.3 * -4.0) = Sigmoid(3.1) = .96\n",
    " * H3 = Sigmoid(0.5 * 1.0 + 0.9 * -1.0 + -0.3 * 0.0) = Sigmoid(-0.4) = .40\n",
    " \n",
    "### Output Layer\n",
    "\n",
    " * O1 = Sigmoid(.13 * -3.0 + .96 * 1.0 + .40 * -3.0) = Sigmoid(-.63) = .35\n",
    " * O2 = Sigmoid(.13 * 0.0 + .96 * 1.0 + .40 * 2.0) = Sigmoid(1.76) = .85\n",
    "\n",
    "\n",
    "In terms of Linear Algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer Matrix\n",
    "H=np.array([[1,-2,2],[2,1,-4],[1,-1,0]],dtype=float)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector\n",
    "inp=np.array([0.5,0.9,-0.3]).reshape(3)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Hidden layer operating over the input\n",
    "Hdotinp=np.dot(H,inp)\n",
    "Hdotinp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "af=expit(Hdotinp)\n",
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output matrix\n",
    "O=np.array([[-3.0, 1.0, -3.0],[0.0, 1.0, 2.0]])\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OdotAf=np.dot(O,af)\n",
    "OdotAf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expit(OdotAf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that we can describe the problem in terms of Linear Algebra is one of the reasons why Neural Networks are so efficient on GPUs. The same operation as a single execution line looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biases\n",
    "\n",
    "It is also very useful to be able to offset our inputs by some constant. \n",
    "You can think of this as centering the activation function or translating the solution (next slide). \n",
    "We will call this constant the bias, and there will often be one value per layer.\n",
    "\n",
    "Our math for the previously calculated layer now looks like this with b=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expit(np.dot(O,expit(np.dot(H,inp) + np.full((3,), 0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for Non-Linearity\n",
    "\n",
    "Neural networks are so effective in classification and regression due to their ability to combine linear and non-linear operation on each step of the evaluation.\n",
    "\n",
    " * The matrix multiply provides the skew and scale.\n",
    " * The bias provides the translation.\n",
    " * The activation function provides the twist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks: The back propagation\n",
    "\n",
    "During training, once we have forward propagated the network, we will find that the final output differs from the known output. The weights must need to be modified to produce better results in the next attempt. \n",
    "\n",
    "How do we know which new weights? to use? \n",
    "\n",
    "We want to minimize the error on our training data. \n",
    "Given labeled inputs, select weights that generate the smallest average error on the outputs.\n",
    "We know that the output is a function of the weights: \n",
    "\n",
    "\n",
    "$$E(w_1,w_2,w_3,...i_1,...t_1,...)$$\n",
    "\n",
    "Just remember that the response of a single neuron can be written as\n",
    "\n",
    "$$f (b + \\sum_{i=1}^N a_i w_i),$$ \n",
    "\n",
    "where the $a_i$ is the output of the previous layer (or the input if it is the second layer) and $w_i$ are the weights. \n",
    "So to figure out which way, we need to change any particular weight, say $w_3$, we want to calculate\n",
    "\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\{w,i,t\\}}$$\n",
    "\n",
    "\n",
    "If we use the chain rule repeatedly across layers we can work our way backward from the output error through the weights, adjusting them as we go. Note that this is where the requirement that activation functions must have nicely behaved derivatives comes from.\n",
    "\n",
    "This technique makes the weight inter-dependencies much more tractable. \n",
    "An elegant perspective on this can be found from [Chris Olahat Blog](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "With basic calculus, you can readily work through the details. \n",
    "\n",
    "You can find an excellent explanation from the renowned [3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the back propagation efficiently\n",
    "\n",
    "The explicit solution for backpropagation leaves us with potentially many millions of simultaneous equations to solve (real nets have a lot of weights). \n",
    "\n",
    "They are non-linear to boot. Fortunately, this isn't a new problem created by deep learning, so we have options from the world of numerical methods.\n",
    "\n",
    "The standard has been **Gradient Descent** local minimization algorithms.\n",
    "\n",
    "\n",
    "To improve the convergence of Gradient Descent, refined methods use adaptive **time step** and incorporate **momentum** to help get over a local minimum. **Momentum** and **step size** are the two hyperparameter\n",
    "\n",
    "The optimization problem that Gradient Descent solves is a local minimization. We don't expect to ever find the actual global minimum. Several techniques have been created to avoid a solution being trapped in a local minima.\n",
    "\n",
    "We could/should find the error for all the training data before updating the weights (an epoch). However, it is usually much more efficient to use a stochastic approach, sampling a random subset of the data, updating the weights, and then repeating with another. This is the **mini-batch Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "A convolutional neural network (CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets can learn these filters/characteristics.\n",
    "\n",
    "![CNN](./fig/cnn.jpeg)\n",
    "\n",
    "\n",
    "As seen from this figure, CNN consists of several convolutional and subsampling layers optionally followed by fully connected layers. \n",
    "\n",
    "Let us say that our input to the convolutional layer is a $m \\times m \\times r$ pixels in an image where $m$ is the height and width of the image and $r$ is the number of channels, e.g. an RGB image has $r=3$. The convolutional layer will have $k$ filters (or kernels) of size $n \\times n \\times q$ where n is smaller than the dimension of the image and $q$ can either be the same as the number of channels r or smaller and may vary for each kernel. The size of the filters gives rise to the locally connected structure which is each convolved with the image to produce k feature maps of size $m−n+1$. \n",
    "\n",
    "A simple demonstration is shown in the figure below, where we assume a binary picture and a single filter of a 3x3 matrix. The primary purpose of Convolution is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. The orange square slide over the figure and for each 3x3 overlap, I multiply every element of the 3x3 submatrix of the figure with the convolution and then I add all elements afterward. \n",
    "\n",
    "![ConvNet](./fig/ConvNet.jpeg)\n",
    " \n",
    " \n",
    " It is clear that different values of the filter matrix will produce different Feature Maps for the same input image.\n",
    " \n",
    " Typical filter matrices are now described. \n",
    " \n",
    " For edge detection:\n",
    " $\n",
    "\\begin{bmatrix}\n",
    "1&0&-1\\\\\n",
    "0&0&0\\\\\n",
    "-1&0&1\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "0&1&0\\\\\n",
    "1&-4&1\\\\\n",
    "0&1&0\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "-1&-1&-1\\\\\n",
    "-1&8&-1\\\\\n",
    "-1&-1&-1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For sharpen:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0&-1&0\\\\\n",
    "-1&5&-1\\\\\n",
    "0&-1&0\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "In practice, a CNN learns the values of these filters on its own during the training process (although we still need to specify parameters such as the number of filters, filter size, architecture of the network, etc. before the training process). The more filters we have, the more image features get extracted, and the better our network becomes at recognizing patterns in unseen images.\n",
    "\n",
    "The other step that is described in this section is the pooling. Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Network\n",
    "\n",
    "Before I end and get more into Neural networks and different packages, I would like to discuss one of the most recent proposals in the literature. The so-called Graph Neural Network. \n",
    "\n",
    "As much of the available information in fields like social network, knowledge graph, recommender system, and even life science comes in the form of graphs, very recently people have developed specific neural networks for these types of applications. Most of the discussion here has been taken from [Zhou's paper](https://arxiv.org/pdf/1812.08434.pdf).\n",
    "\n",
    "A Graph Neural Network is a type of Neural Network which directly operates on the Graph structure, which we define by a set of nodes and edges $G = (V, E)$. A typical application of GNN is node classification. Essentially, every node in the graph is associated with a label, and we want to predict the label of the nodes without ground truth. Here we describe briefly this application and let the reader search for more information.\n",
    "\n",
    "In the node classification problem setup, each node $V$ is characterized by its feature $x_v$ and associated with a ground-truth label $t_v$. Given a partially labeled graph $G$, the goal is to leverage these labeled nodes to predict the labels of the unlabeled. It learns to represent each node with a $d$ dimensional vector (state) $\\vec{h}_V$ which contains the information of its neighborhood. The state embedding $\\vec{h}_V$ is an $s$-dimension vector of node $V$ and can be used to produce an output $\\vec{o}_V$ such as the node label. Let $f$ be a parametric function, called a local transition function, that is shared among all nodes and updates the node state according to the input neighborhood, and let $g$ be the local output function that describes how the output is produced. Then, $\\vec{h}_V$ and $\\vec{o}_V$ are defined as follows:\n",
    "\n",
    "$$\n",
    "\\vec{h}_V = f ( \\vec{x}_V, \\vec{x}_{CO[V]}, \\vec{h}_{ne[V]}, \\vec{x}_{ne[V]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{o}_V = g(\\vec{h}_V, \\vec{x}_V)\n",
    "$$\n",
    "\n",
    "where $\\vec{x}_V$, $\\vec{x}_{co[V]}$, $\\vec{h}_{ne[V]}$, $\\vec{x}_{ne[V]}$ are the features of $V$, the features of its edges, the states, and the features of the nodes in the neighborhood of $V$, respectively.\n",
    "\n",
    "Let $\\vec{H}$, $\\vec{O}$, $\\vec{X}$, and $\\vec{X}_N$ be the vectors constructed by stacking all the states, all the outputs, all the features, and all the node features, respectively. Then we have a compact form as:\n",
    "$$ \\vec{H} = F (\\vec{H}, \\vec{X}) $$\n",
    "$$ \\vec{O} = G(\\vec{H},\\vec{X}_N) $$\n",
    "where $F$, the global transition function, and $G$, the global output function are stacked versions of f and g for all nodes in a graph, respectively. The value of $\\vec{H}$ is the fixed point of Eq. 3 and is uniquely defined with the assumption that $F$ is a contraction map. Since we are seeking a unique solution for $\\vec{h}_v$, we can apply Banach fixed point theorem and rewrite the above equation as an iteratively update process. Such operation is often referred to as message passing or neighborhood aggregation.\n",
    "$$ \\vec{H}^{t+1} = F (\\vec{H}^t, \\vec{X}) $$\n",
    "where $\\vec{H}$ and $\\vec{X}$ denote the concatenation of all the $\\vec{h}$ and $\\vec{x}$, respectively.\n",
    "The output of the GNN is computed by passing the state h_v as well as the feature x_v to an output function g.\n",
    "$$ \\vec{o} = g(\\vec{h}_V,\\vec{x}_V) $$\n",
    "\n",
    "More details on this methodology can be found in the paper above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks\n",
    "\n",
    "Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”). Introduced in 2014 by Goodfellow and other people. \n",
    "\n",
    "GANs’ potential is huge because they can learn to mimic any distribution of data. That is, GANs can be taught to create worlds eerily similar to our own in any domain: images, music, speech, prose. To understand GANs, you should know how generative algorithms work. Up to now, most of the algorithms we have discussed are the so-called discriminative algorithms, where we try to predict the output from a given set of features. In the Bayesian language, we are trying to predict $P(c_j|x_1,x_2,\\cdots,x_n)$. In GANs, we are concerned with a different idea,\n",
    "We try two features given a certain label. Therefore, here we would like to build $P(x_1,x_2,\\cdots,x_n | c_j)$.\n",
    "\n",
    "The idea in GANs then is to have two neural networks. One is called the generator, which generates features, and the other network, the discriminator evaluates its authenticity, i.e. the discriminator decides whether each instance of data that it reviews belongs to the actual training dataset or not. For example, we try to analyze a book by a great author (for example Garcia Marquez). We could analyze the language used in his texts but for this example, the generator should be able to create words and the discriminator should be able to recognize if these are authentic. The idea of the generator then is to create words that were not created by Garcia Marquez but that the discriminator is unable to distinguish.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones of Neural Networks and Deep Learning Frameworks\n",
    "\n",
    "## Artificial Intelligence is born\n",
    "\n",
    "The term **Artificial Intelligence** (AI) was first coined by John McCarthy in 1956.\n",
    "In the early days, the focus was on hard coding rules that computers can follow via inference.\n",
    "That was called the age on **symbolic AI** and let to the expert systems of the 80s and early 90s.\n",
    "\n",
    "## Deep Learning old names\n",
    "\n",
    "Deep Learning has received different names over time. It was called **Cybernetics** in the 70s, **Connectionism** in the 80s. **Neural Networks** in the 90s and today we call it **Deep Learning**\n",
    "\n",
    "\n",
    "## First DL Frameworks from 2000's to 2012\n",
    "\n",
    "  * First high-level tools such as MATLAB, OpenNN, and Torch. \n",
    "  * They were not tailored specifically for neural network model development or having complex user APIs\n",
    "  * No GPU support. \n",
    "  * Machine Learning practitioners do a lot of manual manipulations for the input and output. \n",
    "  * Limited capability for complex networks.\n",
    "  \n",
    "## AlexNet in 2012\n",
    "\n",
    "  * In 2012, Alex Krizhevsky et al. from the University of Toronto proposed a deep neural network architecture later known as [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) that achieved impressive accuracy on ImageNet dataset and outperformed the second-place contestant by a large margin. This outstanding result sparked excitement in deep neural networks after a long time of being considered the\n",
    "  \n",
    "      * *Alex Krizhevsky et al.,* [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (2012), NeurIPS 2012\n",
    "  \n",
    "  * Deep Learning frameworks were born such as **Caffe**, **Chainer** and **Theano**. \n",
    "  * Users could conveniently build more complex deep neural network models such as CNN, RNN, and LSTM. \n",
    "  * Single GPU first and soon after multi-GPU training was supported which significantly reduced the time to train these models and enabled training large models that were not able to fit into a single GPU memory earlier. \n",
    "  * **Caffe** and **Theano** used a declarative programming style while **Chainer** adopted the imperative programming style. \n",
    "  \n",
    "## Big Tech companies jump in\n",
    "\n",
    "  * After the success of **AlexNet** drew great attention in the area of computer vision and reignited the hope of neural networks, large tech companies joined the force of developing deep learning frameworks. \n",
    "  * **Google** open sourced the famous **TensorFlow** framework that is still the most popular deep learning framework in the ML field up to date. \n",
    "  * **Facebook** hired the inventor of Caffe and continued the release of **Caffe2**; at the same time, Facebook AI Research (FAIR) team also released another popular framework **PyTorch** which was based on the Torch framework but with the more popular Python APIs. \n",
    "  * **Microsoft** Research developed the **CNTK** framework. \n",
    "  * **Amazon** adopted **MXNet** from Apache, a joint academic project from the University of Washington, CMU, and others.\n",
    "  \n",
    "  * TensorFlow and CNTK borrowed the declarative programming style from Theano whereas **PyTorch** inherited the intuitive and user-friendly imperative programming style from Torch. \n",
    "  * While *imperative programming* style is more flexible (such as defining a while loop etc.) and easy to trace, *declarative programming* style often provides more room for memory and runtime optimization based on computing graph. \n",
    "  * MXNet, dubbed as “mix”-net, enjoyed the benefits of both worlds by supporting both a set of symbolic (declarative) APIs and a set of imperative APIs at the same time and optimized the performance of models described using imperative APIs via a method called hybridization.\n",
    "  \n",
    "## ResNet in 2016\n",
    "\n",
    "  * In 2015 ResNet was proposed by Kaiming He et al. and again pushed the boundary of image classification by setting another record in ImageNet accuracy. \n",
    "    * *Kaiming He et al.,* [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (2016), CVPR 2016\n",
    "\n",
    "  * Industry and academia realized that deep learning was going to be the next big technology trend to solve challenges in various fields that were not deemed possible before. \n",
    "  * Deep Learning frameworks were polished to provide clear-defined user APIs, optimized for multi-GPU training and distributed training and spawned many model zoos and toolkits that were targeted to specific tasks such as computer vision, natural language processing, etc. \n",
    "  * François Chollet almost single-handedly developed the Keras framework that provides a more intuitive high-level abstraction of neural networks and building blocks on top of existing frameworks such as TensorFlow and MXNet. \n",
    "  * Keras became the de facto model level APIs in TensorFlow 2.x.\n",
    "\n",
    "## Consolidating Deep Learning Frameworks\n",
    "\n",
    "  * Around 2019 start a period of consolidation in the area of Deep Learning frameworks with the duopoly of two big “empires”: TensorFlow and PyTorch.\n",
    "  * PyTorch and TensorFlow represent more than 90% of the use cases of deep learning frameworks in research and production. \n",
    "  * **Theano**, primarily developed by the Montreal Institute for Learning Algorithms (MILA) at the Université de Montréal is no longer actively developed and the last version is from 2020.\n",
    "  * **Chainer** team transitioned their development effort to PyTorch in 2019.\n",
    "  * Microsoft stopped active development of the **CNTK** framework and part of the team moved to support PyTorch on Windows and ONNX runtime. \n",
    "  * **Keras** was assimilated by TensorFlow and became one of its high-level APIs in the TensorFlow 2.0 release. \n",
    "  * **MXNet** remained a distant third in the deep learning framework space.\n",
    "  \n",
    "## Doing Deep Learning at large\n",
    "\n",
    "  * Large model training is a new trend in 2020 to our days.\n",
    "  * With the birth of BERT [3] and its Transformer-based relatives such as GPT-3 [4], the ability to train large models became a desired feature of deep learning frameworks. \n",
    "      * Jacob Devlin et al., [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018)\n",
    "      * Tom B. Brown et al., [Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf) (2020), NeurIPS 2020\n",
    "\n",
    "  * Now training takes place at a scale up to hundreds if not thousands of GPU devices and custom accelerators.       * Deep learning frameworks adopted the imperative programming style for their flexible semantics and easy debugging. \n",
    "  * To compensate for performance frameworks also provide user-level decorators or APIs to achieve high performance through some JIT (just-in-time) compiler techniques.\n",
    "  \n",
    "## Current trends\n",
    "\n",
    "  * **Compiler-based operator optimization**: Operator kernels are usually implemented manually or via some third-party libraries such as BLAS, CuDNN, or OneDNN. There are challenges associated with the movement from development to production. In addition, the growth of new deep learning algorithms is often much faster than the iteration of these libraries making new operators often not supported by these libraries. \n",
    "  * **Deep learning compilers**: Apache TVM, MLIR, Facebook Glow, etc. have been proposed to optimize and run computations efficiently on any hardware backend. They are well positioned to serve as the entire backend stack in the deep learning frameworks.\n",
    "  * **Unified API standards**: Since the majority of machine learning practitioners and data scientists are familiar with the NumPy library, this is the de-facto standard for multidimensional array manipulation. Still, at other levels, many deep learning frameworks share similar but slightly different user APIs.\n",
    "    [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research. It likely became a superset of NumPy with agnostic operation on many devices.\n",
    "  * **Transparent Multi-node or multi-device training**: New frameworks such as [OneFlow](https://docs.oneflow.org/en/master/index.html), treat data communication as part of the overall computation graph of the model training. Data movement optimization is central for multiple training strategies (single device vs multi-device vs distributed training) as the previous deep learning frameworks do, it can provide a simpler user interface with better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "There are many books about Deep Learning and many more on Machine Learning. \n",
    "This list is by no means an exhaustive list of books. I am listing the books from which I took inspiration. Also, I am listing materials where I found better ways to present topics. Often I am amazed by how people can create approachable materials for seemingly dry subjects.\n",
    "\n",
    "The order of the books goes from divulgation and practical to the more rigorous and mathematical. Slides, blogs, and videos are those I have found over the internet or suggested by others.\n",
    "\n",
    "### Selection of Books on Deep Learning\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Kelleher\" \n",
    "       src=\"./fig/books/Deep Learning - Kelleher.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      John D. Kelleher<br>\n",
    "      2019<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Charniak\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Charniak.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Eugene Charniak<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Skansi\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Skansi.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Sandro Skansi<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Subramanian\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Subramanian.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Vishnu Subramanian<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Stevens\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Stevens.png\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Eli Stevens, Luca Artiga and Thomas Viehmann<br>\n",
    "      2020<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with Python - Chollet\" \n",
    "       src=\"./fig/books/Deep Learning with Python - Chollet.jpg\" \n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with Python (Second Edition)<br>\n",
    "      François Chollet<br>\n",
    "      2021<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Patterson\" \n",
    "       src=\"./fig/books/Deep Learning - Patterson.jpeg\"\n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning, a practitioner's approach<br>\n",
    "      Josh Patterson and Adam Gibson<br>\n",
    "      2017<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Goodfellow\" \n",
    "       src=\"./fig/books/Deep Learning - Goodfellow.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      Ian Goodfellow, Yoshua Bengio, and Aaron Courville<br>\n",
    "      2016<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### Interactive Books\n",
    "\n",
    "  * [Dive into Deep Learning](https://d2l.ai/index.html)<br>\n",
    "    Interactive deep learning book with code, math, and discussions<br> \n",
    "    Implemented with PyTorch, NumPy/MXNet, and TensorFlow<br>\n",
    "    Adopted at 300 universities from 55 countries\n",
    "\n",
    "\n",
    "### Slides\n",
    "\n",
    "  * John Urbanic's [\"Deep Learning in one Afternoon\"](https://www.psc.edu/wp-content/uploads/2022/04/Deep-Learning.pdf)<br>\n",
    "An excellent fast, condensed introduction to Deep Learning.<br>\n",
    "John is a Parallel Computing Scientist at Pittsburgh Supercomputing Center\n",
    "\n",
    "  * [Christopher Olah's Blog](http://colah.github.io) is very good. For example about [Back Propagation](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "  * Adam W. Harley on his CMU page offers [An Interactive Node-Link Visualization of Convolutional Neural Networks](https://www.cs.cmu.edu/~aharley/vis/)\n",
    "\n",
    "\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelien Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml) and\n",
    "   [Second Edition](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " * [3Blue1Brown Youtube Channel](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = chapter_number\n",
    "t = np.linspace(0, (2*(n-1)+1)*np.pi/2, 1000)\n",
    "x = t*np.cos(t)**3\n",
    "y = 9*t*np.sqrt(np.abs(np.cos(t))) + t*np.sin(0.3*t)*np.cos(2*t)\n",
    "plt.plot(x, y, c=\"green\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'Chapter {chapter_number} took {int(end - start):d} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
