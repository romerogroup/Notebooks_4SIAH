{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning for Scientists in a hurry](./fig/Title.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGULARITY_NAME=NGC_MXNet_23.03-py3.sif\n",
      "SINGULARITY_ENVIRONMENT=/.singularity.d/env/91-environment.sh\n",
      "SINGULARITY_CONTAINER=/shared/containers/NGC_MXNet_23.03-py3.sif\n",
      "MD_SINGULARITY=/shared/software/singularity/3.7.0\n",
      "SINGULARITY_BIND=\n",
      "SINGULARITY_COMMAND=exec\n"
     ]
    }
   ],
   "source": [
    "!env | grep SIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-07-21T12:33:03.668295-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "Compiler    : GCC 9.4.0\n",
      "OS          : Linux\n",
      "Release     : 3.10.0-1160.24.1.el7.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 52\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "start = time.time()\n",
    "chapter_number = 6\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time, logging, random, math\n",
    "\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gluoncv   : 0.7.0\n",
      "PIL       : 9.4.0\n",
      "mxnet     : 1.9.1\n",
      "logging   : 0.5.1.2\n",
      "matplotlib: 3.6.3\n",
      "argparse  : 1.1\n",
      "numpy     : 1.22.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with CIFAR10 on MXNet\n",
    "\n",
    "In this notebook, we will be training one popular dataset called CIFAR10.\n",
    "\n",
    "This notebook is a modified version from the [Official Documentation for MXNet Gluon](https://cv.gluon.ai/build/examples_classification/dive_deep_cifar10.html)\n",
    "\n",
    "\n",
    "The CIFAR-10 dataset consists of $60000$ $32 \\times 32$ colour images in $10$ classes, with $6000$ images per class. There are $50000$ training images and $10000$ test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    " * airplane\t\t\t\t\t\t\t\t\t\t\n",
    " * automobile\t\t\t\t\t\t\t\t\t\t\n",
    " * bird\t\t\t\t\t\t\t\t\t\t\n",
    " * cat\t\t\t\t\t\t\t\t\t\t\n",
    " * deer\t\t\t\t\t\t\t\t\t\t\n",
    " * dog\t\t\t\t\t\t\t\t\t\t\n",
    " * frog\t\t\t\t\t\t\t\t\t\t\n",
    " * horse\t\t\t\t\t\t\t\t\t\t\n",
    " * ship\t\t\t\t\t\t\t\t\t\t\n",
    " * truck\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, and things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n",
    "\n",
    "\n",
    "## Network Structure\n",
    "\n",
    "\n",
    "\n",
    "Here we pick a simple yet well-performing and still small structure, ``cifar_resnet20_v1``.\n",
    "This is a residual Network with 20 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:33:31] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU\n",
      "[12:33:31] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for GPU\n"
     ]
    }
   ],
   "source": [
    "# Get the model CIFAR_ResNet20_v1, with 10 output classes, without pre-trained weights\n",
    "net = get_model('cifar_resnet20_v1', classes=10)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIFARResNetV1(\n",
       "  (features): HybridSequential(\n",
       "    (0): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "    (2): HybridSequential(\n",
       "      (0): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (1): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (2): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(16 -> 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): HybridSequential(\n",
       "      (0): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(16 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "        (downsample): HybridSequential(\n",
       "          (0): Conv2D(16 -> 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (1): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (2): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): HybridSequential(\n",
       "      (0): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(32 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "        (downsample): HybridSequential(\n",
       "          (0): Conv2D(32 -> 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (1): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "      (2): CIFARBasicBlockV1(\n",
       "        (body): HybridSequential(\n",
       "          (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "          (2): Activation(relu)\n",
       "          (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, act_type=None, bn_group=1, xbuf_ptr=0, in_channels=None)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)\n",
       "  )\n",
       "  (output): Dense(64 -> 10, linear)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at some of the images in CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/opt/mxnet/datasets/cifar10'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DownloadedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"CIFAR10 image classification dataset from https://www.cs.toronto.edu/~kriz/cifar.html\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Each sample is an image (in 3D NDArray) with shape (32, 32, 3).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    root : str, default $MXNET_HOME/datasets/cifar10\u001b[0m\n",
       "\u001b[0;34m        Path to temp folder for storing data.\u001b[0m\n",
       "\u001b[0;34m    train : bool, default True\u001b[0m\n",
       "\u001b[0;34m        Whether to load the training or testing set.\u001b[0m\n",
       "\u001b[0;34m    transform : function, default None\u001b[0m\n",
       "\u001b[0;34m        A user defined callback that transforms each sample. For example::\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            transform=lambda data, label: (data.astype(np.float32)/255, label)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-10-binary.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fab780a1e191a7eda0f345501ccd62d20f7ed891'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_batch_1.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aadd24acce27caa71bf4b10992e9e7b2d74c2540'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0;34m(\u001b[0m\u001b[0;34m'data_batch_2.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c0ba65cce70568cd57b4e03e9ac8d2a5367c1795'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0;34m(\u001b[0m\u001b[0;34m'data_batch_3.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1dd00a74ab1d17a6e7d73e185b69dbf31242f295'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0;34m(\u001b[0m\u001b[0;34m'data_batch_4.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aab85764eb3584312d3c7f65fd2fd016e36a258e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0;34m(\u001b[0m\u001b[0;34m'data_batch_5.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'26e2849e66a845b7f1e4614ae70f4889ae604628'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_batch.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'67eb016db431130d61cd03c7ad570b013799c88c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cifar10'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_read_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3072\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \\\n",
       "               \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_sha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                  \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gluon/dataset/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_namespace\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_repo_file_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_archive_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                                \u001b[0msha1_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_archive_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                            \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0marray_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mx_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_np_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_np_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/mxnet/python/mxnet/gluon/data/vision/datasets.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     CIFAR100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gluon.data.vision.CIFAR10??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = gluon.data.vision.CIFAR10(root=\"./data\",train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 120\n",
    "tens = [ training_data[i][0] for i in range(num_images) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = (1-tensor)*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor, mode='RGB').resize((64,64)) # Doubling the size for visualization\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [ tensor_to_image(tens[i].asnumpy()) for i in range(num_images) ]\n",
    "\n",
    "image_grid(imgs, 8,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Data Loader\n",
    "\n",
    "Data augmentation is a common technique used for training. It is\n",
    "base on the assumption that, for the same object, photos under different\n",
    "composition, lighting condition, or color should all yield the same prediction.\n",
    "\n",
    "\n",
    "We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "input image. Our augmentation transforms the image with\n",
    "resizing, cropping, flipping and other techniques.\n",
    "\n",
    "With ``Gluon``, we can create our transform function as following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area and resize it to be 32x32, then pad it to be 40x40\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that most of the operations are randomized. This in effect\n",
    "increases the number of different images the model sees during training.\n",
    "The more data we have, the better our model generalizes over\n",
    "unseen images.\n",
    "\n",
    "On the other hand, when making prediction, we would like to remove all\n",
    "random operations in order to get a deterministic result. The transform\n",
    "function for prediction is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is important to keep the normalization step, since the\n",
    "model only works well on inputs from the same distribution.\n",
    "\n",
    "With the transform functions, we can define data loaders for our\n",
    "training and validation datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size for Each GPU\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(root=\"./data\",train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(root=\"./data\",train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer, Loss and Metric\n",
    "\n",
    "Optimizer improves the model during training. Here we use the popular\n",
    "Nesterov accelerated gradient descent algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "lr_decay_epoch = [80, 160, np.inf]\n",
    "\n",
    "# Nesterov accelerated gradient descent\n",
    "optimizer = 'nag'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, ``lr_decay`` and ``lr_decay_epoch`` are not directly\n",
    "used in ``trainer``. One important idea in model training is to\n",
    "gradually decrease learning rate. This means the optimizer takes large\n",
    "steps at the beginning, but step size becomes smaller and smaller in time.\n",
    "\n",
    "Our plan sets the learning rate to 0.1 at the beginning, then\n",
    "divide it by 10 at the 80-th epoch, then again at the 160-th epoch.\n",
    "We'll use `lr_decay_epoch` in the main training loop for this purpose.\n",
    "\n",
    "In order to optimize our model, we need a loss function.\n",
    "In essence, loss functions compute the difference between predictions and the\n",
    "ground-truth as a measure of model performance.\n",
    "We can then take the gradients of the loss w.r.t. the weights.\n",
    "Gradients points the optimizer to the direction weights should move to\n",
    "improve model performance.\n",
    "\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics are similar to loss functions, but they are different in the\n",
    "following aspects:\n",
    "\n",
    "-  Metric is how we evaluate model performance. Each metric is related to a\n",
    "   specific task, but independent from the model training process.\n",
    "-  For classification, we usually only use one loss function to train\n",
    "   our model, but we can have several metrics for evaluating\n",
    "   performance.\n",
    "-  Loss function can be used as a metric, but sometimes its values are hard\n",
    "   to interpretate. For instance, the concept \"accuracy\" is\n",
    "   easier to understand than \"softmax cross entropy\"\n",
    "\n",
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validation dataset provides us a way of monitoring the training process.\n",
    "We have labels for validation data, but they are held out during training.\n",
    "Instead, we use them to evaluate the models performance on unseen data\n",
    "and prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate performance, we need a metric. Then, we loop\n",
    "through the validation data and predict with our model.\n",
    "We'll run this function at the end of every epoch to show improvement.\n",
    "over the last epoch.\n",
    "\n",
    "## Training\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 20 epochs.\n",
    "  In your experiments, we recommend setting ``epochs=240``.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Learning rate decay\n",
    "    if epoch == lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1-acc, 1-val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model for 240 epochs, the plot may look like:\n",
    "\n",
    "![Normal Training](./fig/normal_training.png)\n",
    "\n",
    "We can better observe the process of model training with plots.\n",
    "For example, one may ask what will happen if there's no data augmentation:\n",
    "\n",
    "![Overfitting](./fig/overfitting.png)\n",
    "\n",
    "We can see that training error is much lower than validation error.\n",
    "After the model reaches 100\\% accuracy on training data,\n",
    "it stops improving on validation data.\n",
    "These two plots evidently demonstrates the importance of data augmentation.\n",
    "\n",
    "## Model Saving and Loading\n",
    "\n",
    "After training, we usually want to save it for later use.\n",
    "This is simply done with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('./output/dive_deep_cifar10_resnet20_v2.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time if you need to use it, just run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters('./output/dive_deep_cifar10_resnet20_v2.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "There are many books about Deep Learning and many more on Machine Learning. \n",
    "This list is by no means an exhaustive list of books. I am listing the books from which I took inspiration. Also, I am listing materials where I found better ways to present topics. Often I am amazed by how people can create approachable materials for seemingly dry subjects.\n",
    "\n",
    "The order of the books goes from divulgation and practical to the more rigorous and mathematical. Slides, blogs, and videos are those I have found over the internet or suggested by others.\n",
    "\n",
    "### Selection of Books on Deep Learning\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Kelleher\" \n",
    "       src=\"./fig/books/Deep Learning - Kelleher.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      John D. Kelleher<br>\n",
    "      2019<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Charniak\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Charniak.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Eugene Charniak<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Skansi\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Skansi.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Sandro Skansi<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Subramanian\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Subramanian.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Vishnu Subramanian<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Stevens\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Stevens.png\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Eli Stevens, Luca Artiga and Thomas Viehmann<br>\n",
    "      2020<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with Python - Chollet\" \n",
    "       src=\"./fig/books/Deep Learning with Python - Chollet.jpg\" \n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with Python (Second Edition)<br>\n",
    "      François Chollet<br>\n",
    "      2021<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Patterson\" \n",
    "       src=\"./fig/books/Deep Learning - Patterson.jpeg\"\n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning, a practitioner's approach<br>\n",
    "      Josh Patterson and Adam Gibson<br>\n",
    "      2017<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Goodfellow\" \n",
    "       src=\"./fig/books/Deep Learning - Goodfellow.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      Ian Goodfelow, Yoshua Bengio, and Aaron Courville<br>\n",
    "      2016<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### Interactive Books\n",
    "\n",
    "  * [Dive into Deep Learning](https://d2l.ai/index.html)<br>\n",
    "    Interactive deep learning book with code, math, and discussions<br> \n",
    "    Implemented with PyTorch, NumPy/MXNet, and TensorFlow<br>\n",
    "    Adopted at 300 universities from 55 countries\n",
    "\n",
    "\n",
    "### Slides\n",
    "\n",
    "  * John Urbanic's [\"Deep Learning in one Afternoon\"](https://www.psc.edu/wp-content/uploads/2022/04/Deep-Learning.pdf)<br>\n",
    "An excellent fast, condensed introduction to Deep Learning.<br>\n",
    "John is a Parallel Computing Scientist at Pittsburgh Supercomputing Center\n",
    "\n",
    "  * [Christopher Olah's Blog](http://colah.github.io) is very good. For example about [Back Propagation](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "  * Adam W. Harley on his CMU page offers [An Interactive Node-Link Visualization of Convolutional Neural Networks](https://www.cs.cmu.edu/~aharley/vis/)\n",
    "\n",
    "\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelein Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml) and\n",
    "   [Second Edition](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " * [3Blue1Brown Youtube Channel](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = chapter_number\n",
    "t = np.linspace(0, (2*(n-1)+1)*np.pi/2, 1000)\n",
    "x = t*np.cos(t)**3\n",
    "y = 9*t*np.sqrt(np.abs(np.cos(t))) + t*np.sin(0.3*t)*np.cos(2*t)\n",
    "plt.plot(x, y, c=\"green\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'Chapter {chapter_number} took {int(end - start):d} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
