{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Learning for Scientists in a hurry](./fig/Title.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-07-24T20:45:17.671492-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 3.10.0-1160.24.1.el7.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 52\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chapter_number = 1\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 20:45:19.374468: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-24 20:45:21.725411: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy     : 1.22.2\n",
      "tensorflow: 2.12.0+nv23.06\n",
      "matplotlib: 3.7.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_reduction_factor = 1.0\n",
    "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "    print(\"Using TensorFlow CPU only, lowering the number of epochs\")\n",
    "    epoch_reduction_factor = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Dataset of handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset of handwritten digits is a classical problem in Machine Learning and an iconic problem in Convolutional Neural Networks.\n",
    "\n",
    "The MNIST database was constructed from joining two datasets/databases. The NIST's Special Database 3 and Special Database 1. Both datasets contain binary images of handwritten digits. NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found in the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of the training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.\n",
    "\n",
    "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. For the test set, 5,000 patterns were taken from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint.\n",
    "\n",
    "The dataset has a training set of 60,000 examples and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Yann LeCun has a page dedicated to the [MNIST DATASET](http://yann.lecun.com/exdb/mnist) with a list of publications where the dataset has been used.\n",
    "\n",
    "The advantage of using these kinds of datasets is that all the preprocessing and normalization has been done, and some Machine Learning and Deep Learning Frameworks include it with the code or provide easy routines to download it. Let's explore this dataset using TensorFlow and the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path = './data/keras/datasets/mnist.npz'\n",
    "\n",
    "    with np.load(path, allow_pickle=True) as f:  # pylint: disable=unexpected-keyword-arg\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/keras/datasets/mnist.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# **NOTE:** This only works with an internet connection\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# mnist = tf.keras.datasets.mnist\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m (train_images, train_labels), (test_images, test_labels) \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/keras/datasets/mnist.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         x_train, y_train \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m'\u001b[39m], f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m         x_test, y_test \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m'\u001b[39m], f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/keras/datasets/mnist.npz'"
     ]
    }
   ],
   "source": [
    "# **NOTE:** This only works with an internet connection\n",
    "#\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now 4 numpy arrays `train_images`, `train_labels`, `test_images`, and `test_labels`. \n",
    "The shapes of those arrays follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be pointless to look at all those images but we can have a peek into how they look for a small sample of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3\n",
    "M=6\n",
    "fig, axs = plt.subplots(N,M,sharex=True, sharey=True, figsize=(12,6))\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        axs[i,j].imshow(test_images[N*j+i], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a $28 \\times 28$ grayscale bitmap. Let's see one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num=15\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.imshow(test_images[image_num], cmap='gray', interpolation='none')\n",
    "plt.title(r'Digit ${}$'.format(test_labels[image_num]));\n",
    "for i in range(test_images[image_num].shape[0]):\n",
    "    for j in range(test_images[image_num].shape[1]):\n",
    "        if test_images[image_num][i][j]<128:\n",
    "            fontcolor='white'\n",
    "        else:\n",
    "            fontcolor='black'\n",
    "        plt.text(j,i,str(test_images[image_num][i][j]),color=fontcolor, \n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the images into a 1-D vector\n",
    "\n",
    "Our first task is to work out the simplest possible Neural Network model just to show how the TensorFlow framework functions. Then we will gradually implement our way to a quite sophisticated and accurate convolutional neural network for this same problem. \n",
    "\n",
    "First, we need to convert the images into 1D vectors. In NumPy that is called a `reshape`. The data is the same, but it now looks like $60000 \\times 784$ array instead of the $60000 \\times 28 \\times 28$.\n",
    "\n",
    "Another, change is to convert the original values of the array from integers into floating point numbers and divide those by 255 to get a value *normalized* between $0$ and $1$. This is a convenience as many Deep Learning models work better with values in small ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 784) \n",
    "test_images = test_images.reshape(10000, 784)\n",
    "test_images = test_images.astype('float32') \n",
    "train_images = train_images.astype('float32')\n",
    "test_images /= 255 \n",
    "train_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model, our first Dense Neural Network\n",
    "\n",
    "It is time to design the model. We want the network to take $784$ values in the input and feed with them the first layer of $64$ neurons which in turn feed another layer of $64$ to finalize with another layer of $10$. For the activation functions, we will use **ReLU** the function that we saw before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)), \n",
    "            tf.keras.layers.Dense(64, activation='relu'), \n",
    "            tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final activation function will be a **softmax**. The function **softmax** will return 10 values where the sum of them is 1. They will indicate the probability of each input to be associated to one of the 10 digits. \n",
    "\n",
    "The standard (unit) softmax function $\\sigma : \\mathbb{R}^K\\to (0,1)^K$ is defined $K>1$ by the formula\n",
    "\n",
    "$$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\ \\ \\ \\ \\text{ for } i = 1, \\dotsc , K \\text{ and } \\mathbf z=(z_1,\\dotsc,z_K) \\in \\mathbb{R}^K.$$\n",
    "\n",
    "In our case, $K=10$ and **softmax** is exactly the function we need to classify the digits.\n",
    "\n",
    "In simple terms, **softmax** applies an exponential to each element $z_i$ of the vector $\\mathbf z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma(\\mathbf z)$ is 1. This is the reason why we can take the output as the probability of the input image being any of the 10 digits.\n",
    "\n",
    "There is another advantage of using softmax. The values returned at the end of the last layer could be of all sorts, negative values, large and small. softmax reduces all that to all positive, less than 1 set of values. \n",
    "See the table below for some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([ 8.1, -17.5, 10.6, 1.2, -1.5, 13.9,  -3.4, -5.4, 0.05, -2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_z=np.exp(z)\n",
    "exp_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_z=np.exp(z)/np.sum(np.exp(z))\n",
    "sm_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.exp(z)/np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('|{:d}|{:.3f}|{:.3e}|{:.3e}|'.format(i,z[i],exp_z[i],sm_z[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Digit | Ouptut |Exponential | Normalized |\n",
    "| --- | --- | --- | --- |\n",
    "|0|8.100|3.294e+03|2.911e-03|\n",
    "|1|-17.500|2.511e-08|2.219e-14|\n",
    "|2|10.600|4.013e+04|3.547e-02|\n",
    "|3|1.200|3.320e+00|2.934e-06|\n",
    "|4|-1.500|2.231e-01|1.972e-07|\n",
    "|5|13.900|1.088e+06|9.616e-01|\n",
    "|6|-3.400|3.337e-02|2.949e-08|\n",
    "|7|-5.400|4.517e-03|3.991e-09|\n",
    "|8|0.050|1.051e+00|9.290e-07|\n",
    "|9|-2.000|1.353e-01|1.196e-07|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [ str(x) for x in np.arange(10)]\n",
    "values = sm_z\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(211)\n",
    "plt.bar(names, values, log=False);\n",
    "plt.subplot(212)\n",
    "plt.bar(names, values, log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model, defining the backpropagation optimizer, loss function, and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will use the arguments of the `compile` method to organize the procedure that will use to adjust the weights and biases. The values that control the output of our neural network. The optimizer `adam` is a usual choice for many Deep Learning problems. `sparse_categorical_crossentropy` is often used in classification problems like this. Using `accuracy` is a good metric knowing that we have a perfect balance between all the digits present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss function\n",
    "\n",
    "The loss function is the function that we try to minimize and should account for measuring a distance between the desired value and the value returned from the network under current weights and biases.\n",
    "\n",
    "Given the very particular and sensible way we have constructed these outputs.\n",
    "\n",
    "We could be using the Root Mean Square (RMS) as a loss function. The RMS defined as\n",
    "\n",
    "$$\\text{loss}_\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(y_i - y_i^{obj} \\right)^2 }$$.\n",
    "\n",
    "Where $y_i$ are the values obtained from the neural network and $y_i^{obj}$ and the values objective. \n",
    "This function is not a good idea for our case. See on the figure above that except for 1 value all the others are very small, it will be hard for an algorithm to know how in which direction to change the parameters when all produce such small values. We need somehow a way to undo the flattening of the **softmax**.\n",
    "\n",
    "The Cross-Entropy Loss function is a good way to define the error across all possibilities. Better than RMS. \n",
    "The Cross entropy function is defined as:\n",
    "\n",
    "$$\\text{loss}_\\text{CE} =  - \\sum_{i=1}^n y_i^{obj} \\log y_i$$\n",
    "\n",
    "Let's assume that in our case above, we are indeed in the presence of a digit $5$.\n",
    "The value of $y^{obj}$ will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obj=np.zeros(10)\n",
    "y_obj[5]=1\n",
    "y_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network returned these values after the softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the loss function using the equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(sm_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-np.dot(y_obj,np.log(sm_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the value of our loss. This is the value that we want to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Here is where the real processing takes place, the values of weights and biases are changed using the examples provided from the `train_images` and `train_labels`. This is a very high-level routine that is also capable of validating the model with the `test_images` and `test_labels`.\n",
    "\n",
    "We will understand later on how mathematical procedures are used to find the weights and biases until our network return correct answers for the images both in our training and test sets.\n",
    "\n",
    "This is a very small problem so you can train this model on a modern computer in a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dnn1 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=128, \n",
    "                    epochs=int(epoch_reduction_factor*40), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy and loss. The curse of overfitting\n",
    "\n",
    "At the end of $40$ cycles with the training data we got an accuracy of $0.9991$. Very good at least for the training set. \n",
    "\n",
    "However, when we measure the accuracy with the test set. The set of data **unseen** by the model during the fitting the value is reduced to $0.97$.\n",
    "\n",
    "Somehow, our neural network is learning to produce the right results for the values that it knows, but is not as good for the values that have not seen. This is called **overfitting** and to measure it we had to separate the $10000$ examples that we called `test_images` and `test_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'],'o-')\n",
    "    plt.plot(history.history['val_accuracy'],'o-')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_dnn1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar picture we get from the loss function. Notice that we plot the figure below using a semilog plot.\n",
    "The value of the loss function is decreasing, with some degradations beyond epoch 15.\n",
    "\n",
    "However, it is remarkable that even beyond epoch 4, the loss function is not improving with the test set. It is going up over time. Another sign of **overfitting**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.semilogy(history.history['loss'],'o-')\n",
    "    plt.semilogy(history.history['val_loss'],'o-')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_dnn1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you can do better. Still, with the idea of a dense neural network, there are two things we can try.\n",
    "Making the networks thicker or make them deeper. \n",
    "\n",
    "Let's see how far we can go with these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thicker Neural Network\n",
    "\n",
    "In our first network, we use a couple of layers with $64$ neurons in them. What about doubling that figure to $128$?\n",
    "We can think that with more neurons there are more parameters and those extra parameters will offer a richer landscape for optimizing better the weights.\n",
    "\n",
    "As many things in Deep Learning and science in general, arguments, no matter how beautiful ones can be beaten by ugly facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)), \n",
    "            tf.keras.layers.Dense(128, activation='relu'), \n",
    "            tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dnn2 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=128, \n",
    "                    epochs=int(epoch_reduction_factor*40), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_dnn2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_dnn2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not indicative that **bigger** is **better**. \n",
    "\n",
    "We double the number of parameters, from 55 thousand to 118 thousand (See the summary of the model).\n",
    "\n",
    "Neither the accuracy for the test set has been better or the loss function for the test set is lower. We are again stuck with unseen cases that are not well learning. \n",
    "Learning is not about memorizing what you see, learning is also about generalization, prospection, conexions.\n",
    "That is true for our human meaning of \"Learning\" and the machine meaning for \"learning\".\n",
    "\n",
    "What about deeper neural networks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to our original network, but this time instead we will add a couple of extra layers with $64$ neurons each.\n",
    "Will this network start learning *deeper*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)), \n",
    "            tf.keras.layers.Dense(64, activation='relu'), \n",
    "            tf.keras.layers.Dense(64, activation='relu'), \n",
    "            tf.keras.layers.Dense(64, activation='relu'), \n",
    "            tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dnn3 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=128, \n",
    "                    epochs=int(epoch_reduction_factor*40), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_dnn3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_dnn3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, no *thicker* or *deeper* is helping us. With so many parameters, our network is remembering how to answer correctly for the training set but is not learning to generalize the parameters to also answer correctly for unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick detour: Overfitting and the art of fitting elephants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John von Neumann famously said:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite appropriate to raise your attention to the risk of adding more parameters that do not improve anything.\n",
    "\n",
    "Here is a realization of von Neumann's quote on a webpage from [John D Cook](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/). \n",
    "\n",
    "The original code was created by Piotr A. Zolnierczuk based on the paper:\n",
    "\n",
    "    “Drawing an elephant with four complex parameters” \n",
    "    by Jurgen Mayer, Khaled Khairy, and Jonathon Howard,  \n",
    "    Am. J. Phys. 78, 648 (2010), DOI:10.1119/1.3254017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Piotr A. Zolnierczuk (zolnierczukp at ornl dot gov)\n",
    "\n",
    "Based on a paper by:\n",
    "Drawing an elephant with four complex parameters\n",
    "Jurgen Mayer, Khaled Khairy, and Jonathon Howard,\n",
    "Am. J. Phys. 78, 648 (2010), DOI:10.1119/1.3254017\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "# elephant parameters\n",
    "p1, p2, p3, p4 = (50 - 30j, 18 +  8j, 12 - 10j, -14 - 60j )\n",
    "p5 = 40 + 20j # eyepiece\n",
    "\n",
    "def fourier(t, C):\n",
    "    f = np.zeros(t.shape)\n",
    "    A, B = C.real, C.imag\n",
    "    for k in range(len(C)):\n",
    "        f = f + A[k]*np.cos(k*t) + B[k]*np.sin(k*t)\n",
    "    return f\n",
    "\n",
    "def elephant(t, p1, p2, p3, p4, p5):\n",
    "    npar = 6\n",
    "    Cx = np.zeros((npar,), dtype='complex')\n",
    "    Cy = np.zeros((npar,), dtype='complex')\n",
    "\n",
    "    Cx[1] = p1.real*1j\n",
    "    Cx[2] = p2.real*1j\n",
    "    Cx[3] = p3.real\n",
    "    Cx[5] = p4.real\n",
    "\n",
    "    Cy[1] = p4.imag + p1.imag*1j\n",
    "    Cy[2] = p2.imag*1j\n",
    "    Cy[3] = p3.imag*1j\n",
    "\n",
    "    x = np.append(fourier(t,Cx), [-p5.imag])\n",
    "    y = np.append(fourier(t,Cy), [p5.imag])\n",
    "\n",
    "    return x,y\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "x, y = elephant(np.linspace(0,2*np.pi,1000), p1, p2, p3, p4, p5)\n",
    "pylab.plot(y,-x,'.')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No thicker, no deeper, what is next?\n",
    "\n",
    "Lets summarize the results from our 3 networks:\n",
    "\n",
    "| Network Architecture | Number of trainable parameters | Test Lost | Test Accuracy | Validation Loss | Validation Accuracy |\n",
    "|---|---|---|---|---|---|\n",
    "| 64 $\\times$ 64 $\\times$ 10 | 55,050 | 0.0033 | 0.9991 | 0.1379 | 0.9762 |\n",
    "| 128 $\\times$ 128 $\\times$ 10 | 118,282 | 0.0064 | 0.9979 | 0.1261 | 0.9782 | \n",
    "| 64 $\\times$ 64 $\\times$ 64 $\\times$ 64 $\\times$ 10 | 63,370 | 0.0096 | 0.9971 | 0.1556 | 0.9749 |\n",
    "\n",
    "So adding more neurons on a layer or adding more layers is not helping. Where is the problem?\n",
    "\n",
    "For physicists, there is a guiding principle in many areas:\n",
    "\n",
    "      Follow the symmetries!\n",
    "\n",
    "Imagine one of the images in our training set. What if we take the image and we shift the image one pixel.\n",
    "From our eyes (and brain). The image is the same. Translation symmetry is trivial for an image but from the point of view of our neural networks, the original image and the translated image look very different. \n",
    "Many input cells are changed and this is an unfortunate effect of flattening our 2D array into a 1D array. \n",
    "\n",
    "In addition to the flattening, the neural network is not using the grid arrangement of the pixels and the corresponding arrays.\n",
    "\n",
    "These limitations have been addressed by the so-called **Convolutional Neural Networks** (CNN). CNN's are the correct way of doing image recognition with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "A convolutional neural network (CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets can learn these filters/characteristics.\n",
    "\n",
    "## AlexNet\n",
    "\n",
    "AlexNet won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) and revolutionized Deep Learning.\n",
    "\n",
    "![AlexNet1](./fig/AlexNet1.png)\n",
    "\n",
    "![AlexNet2](./fig/AlexNet2.png)\n",
    "\n",
    "\n",
    "## A convolution\n",
    "\n",
    "<!--\n",
    "![Convolution](./fig/convolution.svg)\n",
    "\n",
    "![Convolution 11](./fig/convolution11.svg)\n",
    "\n",
    "![Convolution 23](./fig/convolution23.svg)\n",
    "-->\n",
    "\n",
    "<img src=\"./fig/convolution.svg\" width=500 height=500 />\n",
    "<img src=\"./fig/convolution11.svg\" width=500 height=500 />\n",
    "<img src=\"./fig/convolution23.svg\" width=500 height=500 />\n",
    "\n",
    "![CNN](./fig/cnn.jpeg)\n",
    "\n",
    "\n",
    "As seen from this figure, CNN consists of several convolutional and subsampling layers optionally followed by fully connected layers. \n",
    "\n",
    "Let us say that our input to the convolutional layer is a $m \\times m \\times r$ pixels in an image where $m$ is the height and width of the image and $r$ is the number of channels, e.g. an RGB image has $r=3$. The convolutional layer will have $k$ filters (or kernels) of size $n \\times n \\times q$ where n is smaller than the dimension of the image and $q$ can either be the same as the number of channels r or smaller and may vary for each kernel. The size of the filters gives rise to the locally connected structure which is each convolved with the image to produce k feature maps of size $m−n+1$. \n",
    "\n",
    "A simple demonstration is shown in the figure below, where we assume a binary picture and a single filter of a 3x3 matrix. The primary purpose of Convolution is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. The orange square slide over the figure and for each 3x3 overlap, I multiply every element of the 3x3 submatrix of the figure with the convolution and then I add all elements afterward. \n",
    "\n",
    "![ConvNet](./fig/ConvNet.jpeg)\n",
    " \n",
    " \n",
    " It is clear that different values of the filter matrix will produce different Feature Maps for the same input image.\n",
    " \n",
    " Typical filter matrices are now described. \n",
    " \n",
    " For edge detection:\n",
    " $\n",
    "\\begin{bmatrix}\n",
    "1&0&-1\\\\\n",
    "0&0&0\\\\\n",
    "-1&0&1\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "0&1&0\\\\\n",
    "1&-4&1\\\\\n",
    "0&1&0\\\\\n",
    "\\end{bmatrix}\n",
    "\\;\\;\n",
    "\\begin{bmatrix}\n",
    "-1&-1&-1\\\\\n",
    "-1&8&-1\\\\\n",
    "-1&-1&-1\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "For sharpen:\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "0&-1&0\\\\\n",
    "-1&5&-1\\\\\n",
    "0&-1&0\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "In practice, a CNN learns the values of these filters on its own during the training process (although we still need to specify parameters such as the number of filters, filter size, architecture of the network, etc. before the training process). The more filters we have, the more image features get extracted, and the better our network becomes at recognizing patterns in unseen images.\n",
    "\n",
    "The other step that is described in this section is the pooling. Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classification using a simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = tf.keras.datasets.mnist\n",
    "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "train_images = train_images.reshape(60000, 28, 28, 1) \n",
    "test_images = test_images.reshape(10000, 28, 28, 1) \n",
    "train_images, test_images = train_images/255, test_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.MaxPooling2D(2,2),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)), \n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dense(10, activation='softmax') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cnn1 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=int(epoch_reduction_factor*20), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_cnn1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_cnn1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding another convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)), \n",
    "                            tf.keras.layers.MaxPooling2D(2,2),\n",
    "                            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                            tf.keras.layers.MaxPooling2D(2,2),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dense(10, activation='softmax') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cnn2 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=int(epoch_reduction_factor*20), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_cnn2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_cnn2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dropout\n",
    "\n",
    "It looks counterintuitive but sometimes it is good to prune networks. The technique is called dropout and it is used to reduce overfitting.  \n",
    "\n",
    "When using the Dropout layer, it randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent **overfitting**. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "Note that the Dropout layer only applies during **training**, ie no neurons are dropped during inference. When using `model.fit`, training will be appropriately set to True automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)), \n",
    "                            tf.keras.layers.MaxPooling2D(2,2),\n",
    "                            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                            tf.keras.layers.MaxPooling2D(2,2),\n",
    "                            tf.keras.layers.Dropout(0.25),\n",
    "                            tf.keras.layers.Flatten(),\n",
    "                            tf.keras.layers.Dense(100, activation='relu'),\n",
    "                            tf.keras.layers.Dropout(0.25),\n",
    "                            tf.keras.layers.Dense(10, activation='softmax') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cnn3 = model.fit(train_images, \n",
    "                    train_labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=int(epoch_reduction_factor*20), \n",
    "                    verbose=1, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(hist_cnn3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(hist_cnn3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing all neural networks we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for ihist in [hist_dnn1, hist_dnn2, hist_dnn3, hist_cnn1, hist_cnn2, hist_cnn3]:\n",
    "    plt.plot(ihist.history['val_accuracy'],'o-')\n",
    "\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend([r'DNN $64 \\times 64 \\times 10$', \n",
    "            r'DNN $128 \\times 128 \\times 10$', \n",
    "            r'DNN $64 \\times 64 \\times 64 \\times 64 \\times 10$', \n",
    "            r'CNN $c32mp2 \\times d100 \\times d10$', \n",
    "            r'CNN $c32mp2 \\times c64mp2 \\times d100 \\times d10$', \n",
    "            r'CNN $c32mp2 \\times c64mp2 \\times dp.25 \\times d100 \\times d10$'], loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for ihist in [hist_dnn1, hist_dnn2, hist_dnn3, hist_cnn1, hist_cnn2, hist_cnn3]:\n",
    "    plt.plot(ihist.history['val_loss'],'o-')\n",
    "\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend([r'DNN $64 \\times 64 \\times 10$', \n",
    "            r'DNN $128 \\times 128 \\times 10$', \n",
    "            r'DNN $64 \\times 64 \\times 64 \\times 64 \\times 10$', \n",
    "            r'CNN $c32mp2 \\times d100 \\times d10$', \n",
    "            r'CNN $c32mp2 \\times c64mp2 \\times d100 \\times d10$', \n",
    "            r'CNN $c32mp2 \\times c64mp2 \\times dp.25 \\times d100 \\times d10$'], loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning models using low-level TensorFlow 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, Keras offers an easy and high-level API for building both dense and convolutional Neural Networks with just a few lines of code.\n",
    "\n",
    "At this point is also illustrative how the model can be built explicitly on TensorFlow using the old TensorFlow 1.x API.\n",
    "\n",
    "We start with a set of functions and classes for processing the original MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import numpy\n",
    "import tempfile\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "def maybe_download(filename, work_directory, data_directory):\n",
    "    \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "    if not os.path.exists(work_directory):\n",
    "        os.mkdir(work_directory)\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    filepath2 = os.path.join(work_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    shutil.copyfile(filepath, filepath2)\n",
    "    return filepath2\n",
    "\n",
    "def _read32(bytestream):\n",
    "    dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "    return numpy.frombuffer(bytestream.read(4), dtype=dt)\n",
    "\n",
    "\n",
    "def extract_images(filename):\n",
    "    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2051:\n",
    "            raise ValueError(\n",
    "                'Invalid magic number %d in MNIST image file: %s' %\n",
    "                (magic, filename))\n",
    "        num_images = _read32(bytestream)[0]\n",
    "        rows = _read32(bytestream)[0]\n",
    "        cols = _read32(bytestream)[0]\n",
    "        print(f\"Detected {num_images} images of size {rows} x {cols}\")\n",
    "        buf = bytestream.read(rows * cols * num_images)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "        data = data.reshape(num_images, rows, cols, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def extract_labels(filename, one_hot=False):\n",
    "    \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2049:\n",
    "            raise ValueError(\n",
    "                'Invalid magic number %d in MNIST label file: %s' %\n",
    "                (magic, filename))\n",
    "        num_items = _read32(bytestream)[0]\n",
    "        buf = bytestream.read(num_items)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "        if one_hot:\n",
    "            return dense_to_one_hot(labels)\n",
    "        return labels\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels, fake_data=False):\n",
    "        if fake_data:\n",
    "            self._num_examples = 10000\n",
    "        else:\n",
    "            assert images.shape[0] == labels.shape[0], (\n",
    "                \"images.shape: %s labels.shape: %s\" % (images.shape,\n",
    "                                                       labels.shape))\n",
    "            self._num_examples = images.shape[0]\n",
    "            # Convert shape from [num examples, rows, columns, depth]\n",
    "            # to [num examples, rows*columns] (assuming depth == 1)\n",
    "            assert images.shape[3] == 1\n",
    "            images = images.reshape(images.shape[0],\n",
    "                                    images.shape[1] * images.shape[2])\n",
    "            # Convert from [0, 255] -> [0.0, 1.0].\n",
    "            images = images.astype(numpy.float32)\n",
    "            images = numpy.multiply(images, 1.0 / 255.0)\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, fake_data=False):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        if fake_data:\n",
    "            fake_image = [1.0 for _ in xrange(784)]\n",
    "            fake_label = 0\n",
    "            return [fake_image for _ in xrange(batch_size)], [\n",
    "                fake_label for _ in xrange(batch_size)]\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = numpy.arange(self._num_examples)\n",
    "            numpy.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def read_data_sets(train_dir, data_dir, fake_data=False, one_hot=False):\n",
    "    class DataSets(object):\n",
    "        pass\n",
    "    data_sets = DataSets()\n",
    "    if fake_data:\n",
    "        data_sets.train = DataSet([], [], fake_data=True)\n",
    "        data_sets.validation = DataSet([], [], fake_data=True)\n",
    "        data_sets.test = DataSet([], [], fake_data=True)\n",
    "        return data_sets\n",
    "    TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "    TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "    TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "    TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "    VALIDATION_SIZE = 5000\n",
    "    local_file = maybe_download(TRAIN_IMAGES, train_dir, data_dir)\n",
    "    train_images = extract_images(local_file)\n",
    "    local_file = maybe_download(TRAIN_LABELS, train_dir, data_dir)\n",
    "    train_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "    local_file = maybe_download(TEST_IMAGES, train_dir, data_dir)\n",
    "    test_images = extract_images(local_file)\n",
    "    local_file = maybe_download(TEST_LABELS, train_dir, data_dir)\n",
    "    test_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "    validation_images = train_images[:VALIDATION_SIZE]\n",
    "    validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "    train_images = train_images[VALIDATION_SIZE:]\n",
    "    train_labels = train_labels[VALIDATION_SIZE:]\n",
    "    data_sets.train = DataSet(train_images, train_labels)\n",
    "    data_sets.validation = DataSet(validation_images, validation_labels)\n",
    "    data_sets.test = DataSet(test_images, test_labels)\n",
    "    return data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.mkdtemp()\n",
    "print(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network with TensorFlow 1.x API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building MNIST dataset\")\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "mnist = read_data_sets(tempdir, \"./data/MNIST/raw\", one_hot = True)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "n_nodes_hl4 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "x = tf.compat.v1.placeholder('float', [None, 784])\n",
    "y = tf.compat.v1.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    print(\"Building Neural Network Model\")\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random.normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random.normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random.normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random.normal([n_nodes_hl3]))}\n",
    "\n",
    "    hidden_4_layer = {'weights':tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4])),\n",
    "                      'biases':tf.Variable(tf.random.normal([n_nodes_hl4]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random.normal([n_nodes_hl4, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random.normal([n_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    l4 = tf.add(tf.matmul(l2,hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.relu(l4)\n",
    "\n",
    "\n",
    "    output = tf.matmul(l4,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    print(\"Training the Neural Network\")\n",
    "    prediction = neural_network_model(x)\n",
    "    # OLD VERSION:\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    # NEW:\n",
    "    input_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=tf.stop_gradient(y))\n",
    "    cost = tf.reduce_mean( input_tensor= input_tensor)\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 10\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # OLD:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        # NEW:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print(f'Epoch {epoch}/{hm_epochs} loss: {epoch_loss:12.4e}')\n",
    "\n",
    "        correct = tf.equal(tf.argmax(input=prediction, axis=1), tf.argmax(input=y, axis=1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(input_tensor=tf.cast(correct, 'float'))\n",
    "        print('Test accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with TensorFlow 1.x API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "\n",
    "mnist = read_data_sets(tempdir, \"./data/MNIST/raw\", one_hot = True)\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = tf.Variable(tf.random.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1,shape=[32]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(input=x_image, filters=W_conv1,strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool2d(input=h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv2 = tf.Variable(tf.random.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(input=h_pool1, filters=W_conv2,strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool2d(input=h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_fc1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]))\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "W_fc2 = tf.Variable(tf.random.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, rate=1 - (keep_prob))\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(input=y_conv,axis=1), tf.argmax(input=y_,axis=1))\n",
    "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "n_epochs=10\n",
    "batch_size=50\n",
    "n_samples=mnist.train.num_examples\n",
    "\n",
    "for i in range(1,n_epochs+1):\n",
    "    print(f'Epoch {i}/{n_epochs}')\n",
    "    for j in range(1,int(n_samples/batch_size)+1):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if j%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(f'{j:4d}/{int(n_samples/batch_size):4d} training accuracy: {train_accuracy:7.4f}')\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"Test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "  1. MNIST is a relatively simple dataset. Try to get the simplest network that gives you a 95% accuracy with the validation data. \n",
    "     Check the number of trainable parameters as your measure of simplicity. Imagine that it will be deployed on very modest hardware and you cannot have the luxury of storing hundreds of thousands of parameters.\n",
    "\n",
    "  1. What happens to the number of parameters if we remove the MaxPooling2D from the network architecture?\n",
    "     Use model.summary() to check for that. Do not try to train such a network.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "There are many books about Deep Learning and many more on Machine Learning. \n",
    "This list is by no means an exhaustive list of books. I am listing the books from which I took inspiration. Also, I am listing materials where I found better ways to present topics. Often I am amazed by how people can create approachable materials for seemingly dry subjects.\n",
    "\n",
    "The order of the books goes from divulgation and practical to the more rigorous and mathematical. Slides, blogs, and videos are those I have found over the internet or suggested by others.\n",
    "\n",
    "### Selection of Books on Deep Learning\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Kelleher\" \n",
    "       src=\"./fig/books/Deep Learning - Kelleher.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      John D. Kelleher<br>\n",
    "      2019<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Charniak\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Charniak.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Eugene Charniak<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Introduction to Deep Learning - Skansi\" \n",
    "       src=\"./fig/books/Introduction to Deep Learning - Skansi.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Introduction to Deep Learning<br>\n",
    "      Sandro Skansi<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Subramanian\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Subramanian.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Vishnu Subramanian<br>\n",
    "      2018<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with PyTorch - Stevens\" \n",
    "       src=\"./fig/books/Deep Learning with PyTorch - Stevens.png\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with PyTorch<br>\n",
    "      Eli Stevens, Luca Artiga and Thomas Viehmann<br>\n",
    "      2020<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning with Python - Chollet\" \n",
    "       src=\"./fig/books/Deep Learning with Python - Chollet.jpg\" \n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning with Python (Second Edition)<br>\n",
    "      François Chollet<br>\n",
    "      2021<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Patterson\" \n",
    "       src=\"./fig/books/Deep Learning - Patterson.jpeg\"\n",
    "       height=\"100\" width=\"100\" />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning, a practitioner's approach<br>\n",
    "      Josh Patterson and Adam Gibson<br>\n",
    "      2017<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"clear: both; display: table;\">\n",
    "  <div style=\"border: none; float: left; width: 200; padding: 5px\">\n",
    "  <img alt=\"Deep Learning - Goodfellow\" \n",
    "       src=\"./fig/books/Deep Learning - Goodfellow.jpg\" \n",
    "       height=\"100\" width=\"100\"  />\n",
    "  </div>\n",
    "  <div style=\"border: none; float: left; width: 800; padding: 5px\">\n",
    "      Deep Learning<br>\n",
    "      Ian Goodfellow, Yoshua Bengio, and Aaron Courville<br>\n",
    "      2016<br>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "### Interactive Books\n",
    "\n",
    "  * [Dive into Deep Learning](https://d2l.ai/index.html)<br>\n",
    "    Interactive deep learning book with code, math, and discussions<br> \n",
    "    Implemented with PyTorch, NumPy/MXNet, and TensorFlow<br>\n",
    "    Adopted at 300 universities from 55 countries\n",
    "\n",
    "\n",
    "### Slides\n",
    "\n",
    "  * John Urbanic's [\"Deep Learning in one Afternoon\"](https://www.psc.edu/wp-content/uploads/2022/04/Deep-Learning.pdf)<br>\n",
    "An excellent fast, condensed introduction to Deep Learning.<br>\n",
    "John is a Parallel Computing Scientist at Pittsburgh Supercomputing Center\n",
    "\n",
    "  * [Christopher Olah's Blog](http://colah.github.io) is very good. For example about [Back Propagation](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "  * Adam W. Harley on his CMU page offers [An Interactive Node-Link Visualization of Convolutional Neural Networks](https://www.cs.cmu.edu/~aharley/vis/)\n",
    "\n",
    "\n",
    "\n",
    "### Jupyter Notebooks\n",
    "\n",
    " * [Yale Digital Humanities Lab](https://github.com/YaleDHLab/lab-workshops)\n",
    " \n",
    " * Aurelien Geron Hands-on Machine Learning with Scikit-learn \n",
    "   [First Edition](https://github.com/ageron/handson-ml) and\n",
    "   [Second Edition](https://github.com/ageron/handson-ml2)\n",
    "   \n",
    " * [A progressive collection notebooks of the Machine Learning course by the University of Turin](https://github.com/rugantio/MachineLearningCourse)\n",
    "   \n",
    " * [A curated set of jupyter notebooks about many topics](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n",
    "   \n",
    "### Videos\n",
    "\n",
    " * [Caltech's \"Learning from Data\" by Professor Yaser Abu-Mostafa](https://work.caltech.edu/telecourse.html)\n",
    " \n",
    " * [3Blue1Brown Youtube Channel](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back of the Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = chapter_number\n",
    "t = np.linspace(0, (2*(n-1)+1)*np.pi/2, 1000)\n",
    "x = t*np.cos(t)**3\n",
    "y = 9*t*np.sqrt(np.abs(np.cos(t))) + t*np.sin(0.3*t)*np.cos(2*t)\n",
    "plt.plot(x, y, c=\"green\")\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f'Chapter {chapter_number} took {int(end - start):d} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
